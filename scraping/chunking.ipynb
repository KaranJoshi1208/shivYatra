{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d97be0",
   "metadata": {},
   "source": [
    "## Optimized Tourism Data Chunking\n",
    "\n",
    "Custom Intelligent chunking system for tourism data optimized for RAG systems and small language models (Qwen2.5-1.5B size models). \n",
    "\n",
    "**Features**\n",
    "-  Location hierarchy with GPS coordinates\n",
    "-  Enhanced category/subcategory classification  \n",
    "-  Automatic price range detection\n",
    "-  Traveler-type relevance scoring\n",
    "-  Seasonal content detection\n",
    "-  Multiple output formats for different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a882a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_section(section_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Categorize section by content type for better organization\n",
    "    \"\"\"\n",
    "    section_lower = section_name.lower()\n",
    "    \n",
    "    # Accommodation related\n",
    "    if any(keyword in section_lower for keyword in ['sleep', 'hotel', 'accommodation', 'stay']):\n",
    "        return 'accommodation'\n",
    "    \n",
    "    # Food and dining\n",
    "    elif any(keyword in section_lower for keyword in ['eat', 'drink', 'food', 'restaurant', 'dining']):\n",
    "        return 'dining'\n",
    "    \n",
    "    # Transportation\n",
    "    elif any(keyword in section_lower for keyword in ['get there', 'get around', 'transport', 'bus', 'train', 'flight']):\n",
    "        return 'transport'\n",
    "    \n",
    "    # Activities and sightseeing\n",
    "    elif any(keyword in section_lower for keyword in ['see', 'do', 'activity', 'attraction', 'temple', 'trek']):\n",
    "        return 'activities'\n",
    "    \n",
    "    # Practical information\n",
    "    elif any(keyword in section_lower for keyword in ['money', 'budget', 'cost', 'price', 'phone', 'internet']):\n",
    "        return 'practical'\n",
    "    \n",
    "    # Safety and health\n",
    "    elif any(keyword in section_lower for keyword in ['safe', 'health', 'emergency', 'hospital']):\n",
    "        return 'safety'\n",
    "    \n",
    "    # General information\n",
    "    elif any(keyword in section_lower for keyword in ['introduction', 'understand', 'history', 'culture']):\n",
    "        return 'general'\n",
    "    \n",
    "    else:\n",
    "        return 'misc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319d0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price_info(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract price information from text for price-aware chunking\n",
    "    \"\"\"\n",
    "    price_patterns = [\n",
    "        r'‚Çπ[\\d,]+',  # Indian Rupees\n",
    "        r'Rs\\.?\\s*[\\d,]+',  # Rs format\n",
    "        r'\\$[\\d,]+',  # US Dollars\n",
    "        r'[\\d,]+\\s*rupees?',  # Written rupees\n",
    "        r'cost[s]?\\s*[\\d,‚Çπ\\$]+',  # Cost mentions\n",
    "        r'price[s]?\\s*[\\d,‚Çπ\\$]+',  # Price mentions\n",
    "    ]\n",
    "    \n",
    "    prices = []\n",
    "    for pattern in price_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        prices.extend(matches)\n",
    "    \n",
    "    return prices\n",
    "\n",
    "def extract_location_info(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract location references for location-aware chunking\n",
    "    \"\"\"\n",
    "    location_patterns = [\n",
    "        r'near\\s+[\\w\\s]+',\n",
    "        r'close\\s+to\\s+[\\w\\s]+', \n",
    "        r'[\\d]+\\s*km\\s+from\\s+[\\w\\s]+',\n",
    "        r'located\\s+[\\w\\s]+',\n",
    "        r'station\\s*[\\w\\s]*',\n",
    "        r'airport\\s*[\\w\\s]*',\n",
    "        r'mall\\s+road',\n",
    "        r'main\\s+market'\n",
    "    ]\n",
    "    \n",
    "    locations = []\n",
    "    for pattern in location_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        locations.extend([match.strip() for match in matches])\n",
    "    \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Schema Functions - Location and Classification\n",
    "from datetime import datetime\n",
    "\n",
    "# GPS coordinates for major Indian tourist destinations\n",
    "CITY_COORDINATES = {\n",
    "    # Himachal Pradesh\n",
    "    'manali': [32.2432, 77.1892],\n",
    "    'shimla': [31.1048, 77.1734],\n",
    "    'dharamsala': [32.2190, 76.3234],\n",
    "    'dalhousie': [32.5448, 75.9715],\n",
    "    'kasol': [32.0102, 77.2953],\n",
    "    'kullu': [31.9578, 77.1176],\n",
    "    \n",
    "    # Jammu & Kashmir\n",
    "    'srinagar': [34.0837, 74.7973],\n",
    "    'gulmarg': [34.0484, 74.3831],\n",
    "    'pahalgam': [34.0158, 75.3312],\n",
    "    'sonamarg': [34.2996, 75.2912],\n",
    "    'katra': [32.9616, 74.9329],\n",
    "    'jammu': [32.7266, 74.8570],\n",
    "    \n",
    "    # Ladakh\n",
    "    'leh': [34.1526, 77.5771],\n",
    "    'kargil': [34.5539, 76.1312],\n",
    "    'nubra valley': [34.5240, 77.6025],\n",
    "    \n",
    "    # Uttarakhand\n",
    "    'dehradun': [30.3165, 78.0322],\n",
    "    'mussoorie': [30.4598, 78.0664],\n",
    "    'nainital': [29.3803, 79.4636],\n",
    "    'rishikesh': [30.0869, 78.2676],\n",
    "    'haridwar': [29.9457, 78.1642],\n",
    "    'kedarnath': [30.7346, 79.0669],\n",
    "    'badrinath': [30.7433, 79.4938],\n",
    "    'almora': [29.5971, 79.6593],\n",
    "    'mukteshwar': [29.4779, 79.6425],\n",
    "    \n",
    "    # General fallbacks\n",
    "    'himachal pradesh': [31.1048, 77.1734],\n",
    "    'jammu and kashmir': [34.0837, 74.7973], \n",
    "    'ladakh': [34.1526, 77.5771],\n",
    "    'uttarakhand': [30.3165, 78.0322],\n",
    "    'india': [20.5937, 78.9629]\n",
    "}\n",
    "\n",
    "def extract_location_info_optimized(destination: str, state: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract structured location information with GPS coordinates\"\"\"\n",
    "    city = destination.lower().strip()\n",
    "    state_name = state.strip() if state else \"Unknown\"\n",
    "    \n",
    "    # Get coordinates\n",
    "    coordinates = CITY_COORDINATES.get(city)\n",
    "    if not coordinates and state_name.lower() in CITY_COORDINATES:\n",
    "        coordinates = CITY_COORDINATES[state_name.lower()]\n",
    "    if not coordinates:\n",
    "        coordinates = CITY_COORDINATES['india']  # Default fallback\n",
    "    \n",
    "    return {\n",
    "        \"country\": \"India\",\n",
    "        \"state\": state_name,\n",
    "        \"city\": destination,\n",
    "        \"coordinates\": coordinates\n",
    "    }\n",
    "\n",
    "def classify_content_optimized(section_name: str, content: str, category: str) -> Dict[str, str]:\n",
    "    \"\"\"Enhanced classification with detailed subcategories\"\"\"\n",
    "    section_lower = section_name.lower()\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    subcategory_map = {\n",
    "        'accommodation': {\n",
    "            'budget': ['budget', 'cheap', 'backpacker', 'hostel', 'guesthouse'],\n",
    "            'mid_range': ['mid-range', 'moderate', 'standard'],\n",
    "            'luxury': ['luxury', 'premium', 'resort', 'heritage', 'palace'],\n",
    "            'homestay': ['homestay', 'family', 'local']\n",
    "        },\n",
    "        'dining': {\n",
    "            'street_food': ['street', 'local', 'dhaba', 'roadside'],\n",
    "            'restaurant': ['restaurant', 'fine dining', 'cafe'],\n",
    "            'traditional': ['traditional', 'authentic', 'local cuisine'],\n",
    "            'international': ['pizza', 'chinese', 'continental']\n",
    "        },\n",
    "        'activities': {\n",
    "            'adventure': ['trek', 'rafting', 'paragliding', 'skiing', 'climbing'],\n",
    "            'sightseeing': ['temple', 'palace', 'fort', 'museum', 'monument'],\n",
    "            'nature': ['lake', 'valley', 'peak', 'waterfall', 'garden'],\n",
    "            'cultural': ['festival', 'market', 'local', 'heritage']\n",
    "        },\n",
    "        'transport': {\n",
    "            'road': ['bus', 'taxi', 'car', 'drive'],\n",
    "            'rail': ['train', 'railway', 'station'],\n",
    "            'air': ['flight', 'airport', 'fly'],\n",
    "            'local': ['rickshaw', 'auto', 'local transport']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    subcategory = 'general'\n",
    "    if category in subcategory_map:\n",
    "        for sub, keywords in subcategory_map[category].items():\n",
    "            if any(keyword in content_lower or keyword in section_lower for keyword in keywords):\n",
    "                subcategory = sub\n",
    "                break\n",
    "    \n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"subcategory\": subcategory\n",
    "    }\n",
    "\n",
    "def extract_practical_info_optimized(content: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract practical information: prices, contacts, seasonal data\"\"\"\n",
    "    # Price extraction and categorization\n",
    "    prices = extract_price_info(content)\n",
    "    price_range = \"unknown\"\n",
    "    \n",
    "    if prices:\n",
    "        # Extract numeric values from prices\n",
    "        numeric_prices = []\n",
    "        for price in prices:\n",
    "            nums = re.findall(r'[\\d,]+', price.replace(',', ''))\n",
    "            if nums:\n",
    "                try:\n",
    "                    numeric_prices.append(int(nums[0]))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if numeric_prices:\n",
    "            avg_price = sum(numeric_prices) / len(numeric_prices)\n",
    "            if avg_price < 1000:\n",
    "                price_range = \"budget\"\n",
    "            elif avg_price < 3000:\n",
    "                price_range = \"mid_range\"\n",
    "            else:\n",
    "                price_range = \"luxury\"\n",
    "    \n",
    "    # Contact information\n",
    "    has_contact = bool(re.search(r'\\+91|phone|contact|email|call|booking', content, re.IGNORECASE))\n",
    "    \n",
    "    # Seasonal information\n",
    "    seasonal = []\n",
    "    if re.search(r'winter|snow|skiing|december|january|february', content, re.IGNORECASE):\n",
    "        seasonal.append(\"winter\")\n",
    "    if re.search(r'summer|may|june|july|august', content, re.IGNORECASE):\n",
    "        seasonal.append(\"summer\")\n",
    "    if re.search(r'monsoon|rain|july|august|september', content, re.IGNORECASE):\n",
    "        seasonal.append(\"monsoon\")\n",
    "    if re.search(r'spring|march|april|pleasant', content, re.IGNORECASE):\n",
    "        seasonal.append(\"spring\")\n",
    "    \n",
    "    if not seasonal:\n",
    "        seasonal = [\"all_year\"]\n",
    "    \n",
    "    practical_info = {\n",
    "        \"price_range\": price_range,\n",
    "        \"prices\": prices[:3],  # Keep top 3 prices\n",
    "        \"has_contact\": has_contact,\n",
    "    }\n",
    "    \n",
    "    # Only add seasonal if it's relevant (not all_year)\n",
    "    if seasonal != [\"all_year\"]:\n",
    "        practical_info[\"seasonal\"] = seasonal\n",
    "    \n",
    "    return practical_info\n",
    "\n",
    "def calculate_relevance_scores(content: str, category: str) -> Dict[str, int]:\n",
    "    \"\"\"Calculate relevance scores for different traveler types (1-10 scale)\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Base scores by category\n",
    "    base_scores = {\n",
    "        'accommodation': {'solo_traveler': 6, 'family': 7, 'adventure': 5},\n",
    "        'dining': {'solo_traveler': 7, 'family': 8, 'adventure': 6},\n",
    "        'activities': {'solo_traveler': 7, 'family': 6, 'adventure': 9},\n",
    "        'transport': {'solo_traveler': 8, 'family': 7, 'adventure': 8},\n",
    "        'practical': {'solo_traveler': 9, 'family': 9, 'adventure': 8},\n",
    "        'safety': {'solo_traveler': 8, 'family': 10, 'adventure': 7}\n",
    "    }\n",
    "    \n",
    "    scores = base_scores.get(category, {'solo_traveler': 5, 'family': 5, 'adventure': 5})\n",
    "    \n",
    "    # Adjust scores based on content\n",
    "    if any(word in content_lower for word in ['budget', 'cheap', 'backpacker']):\n",
    "        scores['solo_traveler'] += 2\n",
    "        scores['family'] -= 1\n",
    "    \n",
    "    if any(word in content_lower for word in ['family', 'kids', 'children', 'safe']):\n",
    "        scores['family'] += 2\n",
    "        scores['solo_traveler'] += 1\n",
    "    \n",
    "    if any(word in content_lower for word in ['trek', 'adventure', 'climbing', 'rafting', 'extreme']):\n",
    "        scores['adventure'] += 3\n",
    "        scores['family'] -= 1\n",
    "        \n",
    "    if any(word in content_lower for word in ['luxury', 'premium', 'resort']):\n",
    "        scores['family'] += 1\n",
    "        scores['solo_traveler'] -= 1\n",
    "    \n",
    "    # Cap scores between 1-10\n",
    "    for key in scores:\n",
    "        scores[key] = max(1, min(10, scores[key]))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3c2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smart_chunks(content: str, max_chunk_size: int = 250) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create intelligent chunks optimized for 1.5B model\n",
    "    Split on sentences first, then paragraphs, maintaining context\n",
    "    \"\"\"\n",
    "    if len(content) <= max_chunk_size:\n",
    "        return [content]\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Split by paragraphs first\n",
    "    paragraphs = [p.strip() for p in content.split('\\n') if p.strip()]\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If paragraph alone is too big, split by sentences\n",
    "        if len(paragraph) > max_chunk_size:\n",
    "            sentences = re.split(r'[.!?]+', paragraph)\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                    \n",
    "                # If adding this sentence would exceed limit\n",
    "                if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sentence\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        current_chunk += \". \" + sentence\n",
    "                    else:\n",
    "                        current_chunk = sentence\n",
    "        else:\n",
    "            # If adding this paragraph would exceed limit\n",
    "            if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    current_chunk += \"\\n\" + paragraph\n",
    "                else:\n",
    "                    current_chunk = paragraph\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceafca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file_optimized(file_path: str, state_name: str = \"\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a single JSON file using the new optimized schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    destination = data.get('title', 'Unknown')\n",
    "    sections = data.get('sections', {})\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunk_counter = 1\n",
    "    \n",
    "    for section_name, section_content in sections.items():\n",
    "        if not section_content or len(section_content.strip()) < 20:\n",
    "            continue\n",
    "            \n",
    "        # Get basic category\n",
    "        category = categorize_section(section_name)\n",
    "        \n",
    "        # Create smart chunks\n",
    "        content_chunks = create_smart_chunks(section_content, max_chunk_size=250)\n",
    "        \n",
    "        for i, chunk_content in enumerate(content_chunks):\n",
    "            # Extract location information\n",
    "            location_info = extract_location_info_optimized(destination, state_name)\n",
    "            \n",
    "            # Enhanced classification\n",
    "            classification = classify_content_optimized(section_name, chunk_content, category)\n",
    "            \n",
    "            # Extract practical information\n",
    "            practical_info = extract_practical_info_optimized(chunk_content)\n",
    "            \n",
    "            # Calculate relevance scores\n",
    "            relevance_scores = calculate_relevance_scores(chunk_content, category)\n",
    "            \n",
    "            # Create structured chunk ID\n",
    "            state_code = {\n",
    "                'Himachal': 'hp',\n",
    "                'Jammu': 'jk', \n",
    "                'Ladakh': 'lad',\n",
    "                'Uttarakhand': 'uk',\n",
    "                'India': 'in'\n",
    "            }.get(state_name, state_name.lower()[:2] if state_name else 'in')\n",
    "            \n",
    "            city_code = destination.lower().replace(' ', '_')[:6]\n",
    "            cat_code = classification['category'][:3]\n",
    "            subcat_code = classification['subcategory'][:3]\n",
    "            \n",
    "            chunk_id = f\"in_{state_code}_{city_code}_{cat_code}_{subcat_code}_{chunk_counter:03d}\"\n",
    "            \n",
    "            # Build optimized chunk structure\n",
    "            chunk = {\n",
    "                'chunk_id': chunk_id,\n",
    "                'content': chunk_content,\n",
    "                'location': location_info,\n",
    "                'classification': classification,\n",
    "                'practical_info': practical_info,\n",
    "                'relevance_scores': relevance_scores,\n",
    "                'last_updated': datetime.now().strftime('%Y-%m-%d')\n",
    "            }\n",
    "            \n",
    "            all_chunks.append(chunk)\n",
    "            chunk_counter += 1\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3523826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_all_data_optimized(base_path: str = \"../json_data\") -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Process all JSON files using the new optimized chunking schema\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    all_chunked_data = {}\n",
    "    \n",
    "    # Process main India file\n",
    "    india_file = base_path / \"india.json\"\n",
    "    if india_file.exists():\n",
    "        print(f\"Processing main India file...\")\n",
    "        india_chunks = process_json_file_optimized(str(india_file), \"India\")\n",
    "        all_chunked_data[\"India\"] = india_chunks\n",
    "        print(f\"Created {len(india_chunks)} chunks for India\")\n",
    "    \n",
    "    # Process Him_north folder\n",
    "    him_north_path = base_path / \"Him_north\"\n",
    "    if him_north_path.exists():\n",
    "        \n",
    "        # Process state folders\n",
    "        for state_folder in him_north_path.iterdir():\n",
    "            if state_folder.is_dir():\n",
    "                state_name = state_folder.name.title()\n",
    "                print(f\"\\nProcessing {state_name} state...\")\n",
    "                \n",
    "                state_chunks = []\n",
    "                \n",
    "                # Process all JSON files in state folder\n",
    "                for json_file in state_folder.glob(\"*.json\"):\n",
    "                    file_chunks = process_json_file_optimized(str(json_file), state_name)\n",
    "                    state_chunks.extend(file_chunks)\n",
    "                    print(f\"  {json_file.name}: {len(file_chunks)} chunks\")\n",
    "                \n",
    "                all_chunked_data[state_name] = state_chunks\n",
    "                print(f\"Total chunks for {state_name}: {len(state_chunks)}\")\n",
    "        \n",
    "        # Process files directly in Him_north folder\n",
    "        for json_file in him_north_path.glob(\"*.json\"):\n",
    "            file_chunks = process_json_file_optimized(str(json_file), \"Himalayan_North\")\n",
    "            all_chunked_data[\"Himalayan_North\"] = file_chunks\n",
    "            print(f\"Himalayan North region: {len(file_chunks)} chunks\")\n",
    "    \n",
    "    return all_chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d458d13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting OPTIMIZED chunking process with new schema...\n",
      "======================================================================\n",
      "Processing main India file...\n",
      "Created 1086 chunks for India\n",
      "\n",
      "Processing Himachal state...\n",
      "  dalhousie (india).json: 79 chunks\n",
      "  dharamsala.json: 180 chunks\n",
      "  bilaspur (himachal pradesh).json: 106 chunks\n",
      "  kullu.json: 28 chunks\n",
      "  palampur.json: 30 chunks\n",
      "  mandi.json: 47 chunks\n",
      "  shimla.json: 199 chunks\n",
      "  manali.json: 192 chunks\n",
      "  himachal pradesh.json: 53 chunks\n",
      "  jogindernagar.json: 87 chunks\n",
      "Total chunks for Himachal: 1001\n",
      "\n",
      "Processing Uttarakhand state...\n",
      "  nainital.json: 115 chunks\n",
      "  munsyari.json: 15 chunks\n",
      "  mussoorie.json: 84 chunks\n",
      "  uttarakhand.json: 82 chunks\n",
      "  nanda devi national park.json: 28 chunks\n",
      "  rajaji national park.json: 15 chunks\n",
      "  mukteshwar.json: 22 chunks\n",
      "  haridwar.json: 105 chunks\n",
      "  kedarnath.json: 47 chunks\n",
      "  dunagiri.json: 22 chunks\n",
      "  ghangaria.json: 52 chunks\n",
      "  almora.json: 36 chunks\n",
      "  gangotri.json: 31 chunks\n",
      "  jim corbett national park.json: 50 chunks\n",
      "  badrinath.json: 40 chunks\n",
      "  pithoragarh.json: 31 chunks\n",
      "  chakrata.json: 24 chunks\n",
      "  rishikesh.json: 165 chunks\n",
      "  dehradun.json: 133 chunks\n",
      "Total chunks for Uttarakhand: 1097\n",
      "\n",
      "Processing Jammu state...\n",
      "  srinagar.json: 161 chunks\n",
      "  gulmarg.json: 67 chunks\n",
      "  katra.json: 30 chunks\n",
      "  jammu and kashmir.json: 65 chunks\n",
      "  pahalgam.json: 38 chunks\n",
      "  patnitop.json: 28 chunks\n",
      "  sonamarg.json: 20 chunks\n",
      "  jammu.json: 46 chunks\n",
      "Total chunks for Jammu: 455\n",
      "\n",
      "Processing Ladakh state...\n",
      "  alchi.json: 13 chunks\n",
      "  leh.json: 132 chunks\n",
      "  nubra valley.json: 55 chunks\n",
      "  kargil.json: 30 chunks\n",
      "  lamayuru.json: 12 chunks\n",
      "  hanle.json: 5 chunks\n",
      "  ladakh.json: 203 chunks\n",
      "  changthang western lakes.json: 31 chunks\n",
      "Total chunks for Ladakh: 481\n",
      "Himalayan North region: 40 chunks\n",
      "\n",
      "üìä OPTIMIZED CHUNKING SUMMARY:\n",
      "======================================================================\n",
      "India: 1,086 chunks\n",
      "Himachal: 1,001 chunks\n",
      "Uttarakhand: 1,097 chunks\n",
      "Jammu: 455 chunks\n",
      "Ladakh: 481 chunks\n",
      "Himalayan_North: 40 chunks\n",
      "\n",
      "üéØ TOTAL OPTIMIZED CHUNKS CREATED: 4,160\n",
      "\n",
      "üìà OPTIMIZED CHUNK ANALYTICS:\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  CATEGORY DISTRIBUTION:\n",
      "------------------------------\n",
      "misc        : 2,059 (49.5%)\n",
      "activities  :   764 (18.4%)\n",
      "accommodation:   329 ( 7.9%)\n",
      "dining      :   302 ( 7.3%)\n",
      "general     :   296 ( 7.1%)\n",
      "transport   :   231 ( 5.6%)\n",
      "practical   :   164 ( 3.9%)\n",
      "safety      :    15 ( 0.4%)\n",
      "\n",
      "üîñ SUBCATEGORY DISTRIBUTION:\n",
      "------------------------------\n",
      "general     : 3,212 (77.2%)\n",
      "adventure   :   201 ( 4.8%)\n",
      "sightseeing :   145 ( 3.5%)\n",
      "road        :   138 ( 3.3%)\n",
      "nature      :   110 ( 2.6%)\n",
      "restaurant  :    79 ( 1.9%)\n",
      "street_food :    60 ( 1.4%)\n",
      "rail        :    53 ( 1.3%)\n",
      "luxury      :    44 ( 1.1%)\n",
      "budget      :    38 ( 0.9%)\n",
      "\n",
      "üí∞ PRICE RANGE DISTRIBUTION:\n",
      "------------------------------\n",
      "unknown     : 3,723 (89.5%)\n",
      "budget      :   316 ( 7.6%)\n",
      "mid_range   :    76 ( 1.8%)\n",
      "luxury      :    45 ( 1.1%)\n",
      "\n",
      "‚úÖ Optimized chunking completed successfully!\n",
      "üéâ New schema features: Location hierarchy, relevance scores, structured classification!\n"
     ]
    }
   ],
   "source": [
    "# Execute the NEW OPTIMIZED chunking process\n",
    "print(\"üöÄ Starting OPTIMIZED chunking process with new schema...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "chunked_data_optimized = chunk_all_data_optimized()\n",
    "\n",
    "# Display summary statistics  \n",
    "print(f\"\\nüìä OPTIMIZED CHUNKING SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_chunks = 0\n",
    "for state, chunks in chunked_data_optimized.items():\n",
    "    chunk_count = len(chunks)\n",
    "    total_chunks += chunk_count\n",
    "    print(f\"{state}: {chunk_count:,} chunks\")\n",
    "\n",
    "print(f\"\\nüéØ TOTAL OPTIMIZED CHUNKS CREATED: {total_chunks:,}\")\n",
    "\n",
    "# Analyze optimized chunk statistics\n",
    "all_chunks_flat = []\n",
    "for chunks in chunked_data_optimized.values():\n",
    "    all_chunks_flat.extend(chunks)\n",
    "\n",
    "if all_chunks_flat:\n",
    "    # Category analysis\n",
    "    categories = {}\n",
    "    subcategories = {}\n",
    "    price_ranges = {}\n",
    "    states = {}\n",
    "    \n",
    "    for chunk in all_chunks_flat:\n",
    "        cat = chunk['classification']['category']\n",
    "        subcat = chunk['classification']['subcategory']\n",
    "        price_range = chunk['practical_info']['price_range']\n",
    "        state = chunk['location']['state']\n",
    "        \n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "        subcategories[subcat] = subcategories.get(subcat, 0) + 1\n",
    "        price_ranges[price_range] = price_ranges.get(price_range, 0) + 1\n",
    "        states[state] = states.get(state, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìà OPTIMIZED CHUNK ANALYTICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  CATEGORY DISTRIBUTION:\")\n",
    "    print(\"-\" * 30)\n",
    "    for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total_chunks) * 100\n",
    "        print(f\"{category:12}: {count:5,} ({percentage:4.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîñ SUBCATEGORY DISTRIBUTION:\")\n",
    "    print(\"-\" * 30)\n",
    "    for subcat, count in sorted(subcategories.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        percentage = (count / total_chunks) * 100\n",
    "        print(f\"{subcat:12}: {count:5,} ({percentage:4.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüí∞ PRICE RANGE DISTRIBUTION:\")\n",
    "    print(\"-\" * 30)\n",
    "    for price_range, count in sorted(price_ranges.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total_chunks) * 100\n",
    "        print(f\"{price_range:12}: {count:5,} ({percentage:4.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Optimized chunking completed successfully!\")\n",
    "print(\"üéâ New schema features: Location hierarchy, relevance scores, structured classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7fdfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ SAVING OPTIMIZED CHUNKED DATA...\n",
      "--------------------------------------------------\n",
      "‚úÖ Optimized flat format: optimized_chunks/all_chunks_optimized.json (4,160 chunks)\n",
      "‚úÖ Location organized: optimized_chunks/chunks_by_location.json\n",
      "‚úÖ Category organized: optimized_chunks/chunks_by_category_optimized.json\n",
      "‚úÖ solo_traveler: optimized_chunks/high_relevance_solo_traveler.json (1,544 chunks)\n",
      "‚úÖ family: optimized_chunks/high_relevance_family.json (1,062 chunks)\n",
      "‚úÖ adventure: optimized_chunks/high_relevance_adventure.json (1,295 chunks)\n",
      "‚úÖ Budget focused: optimized_chunks/budget_focused_chunks.json (392 chunks)\n",
      "‚úÖ Practical info: optimized_chunks/practical_info_optimized.json (971 chunks)\n",
      "\n",
      "üìÅ All optimized formats saved to: optimized_chunks\n",
      "\n",
      "üöÄ OPTIMIZED RAG-READY DATA!\n",
      "New enhanced formats:\n",
      "‚Ä¢ all_chunks_optimized.json ‚Üí Vector database with location coordinates\n",
      "‚Ä¢ chunks_by_location.json ‚Üí State/City hierarchical search\n",
      "‚Ä¢ chunks_by_category_optimized.json ‚Üí Activity-based with subcategories\n",
      "‚Ä¢ high_relevance_[type].json ‚Üí Traveler-specific recommendations\n",
      "‚Ä¢ budget_focused_chunks.json ‚Üí Price-conscious travel planning\n",
      "‚Ä¢ practical_info_optimized.json ‚Üí Contact details and pricing\n"
     ]
    }
   ],
   "source": [
    "# Save optimized chunked data in multiple RAG-ready formats\n",
    "def save_optimized_chunked_data(chunked_data: Dict, output_dir: str = \"optimized_chunks\"):\n",
    "    \"\"\"Save optimized chunked data in multiple formats designed for tourism RAG\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüíæ SAVING OPTIMIZED CHUNKED DATA...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Format 1: Flat list of all optimized chunks (for vector database)\n",
    "    all_chunks_flat = []\n",
    "    for chunks in chunked_data.values():\n",
    "        all_chunks_flat.extend(chunks)\n",
    "    \n",
    "    flat_file = output_path / \"all_chunks_optimized.json\"\n",
    "    with open(flat_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_chunks_flat, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Optimized flat format: {flat_file} ({len(all_chunks_flat):,} chunks)\")\n",
    "    \n",
    "    # Format 2: Organized by state and city (for location-based retrieval)\n",
    "    location_organized = {}\n",
    "    for chunk in all_chunks_flat:\n",
    "        state = chunk['location']['state']\n",
    "        city = chunk['location']['city']\n",
    "        \n",
    "        if state not in location_organized:\n",
    "            location_organized[state] = {}\n",
    "        if city not in location_organized[state]:\n",
    "            location_organized[state][city] = []\n",
    "        \n",
    "        location_organized[state][city].append(chunk)\n",
    "    \n",
    "    location_file = output_path / \"chunks_by_location.json\"\n",
    "    with open(location_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(location_organized, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Location organized: {location_file}\")\n",
    "    \n",
    "    # Format 3: Organized by category and subcategory (for activity-based search)\n",
    "    category_organized = {}\n",
    "    for chunk in all_chunks_flat:\n",
    "        category = chunk['classification']['category']\n",
    "        subcategory = chunk['classification']['subcategory']\n",
    "        \n",
    "        if category not in category_organized:\n",
    "            category_organized[category] = {}\n",
    "        if subcategory not in category_organized[category]:\n",
    "            category_organized[category][subcategory] = []\n",
    "            \n",
    "        category_organized[category][subcategory].append(chunk)\n",
    "    \n",
    "    category_file = output_path / \"chunks_by_category_optimized.json\"\n",
    "    with open(category_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(category_organized, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Category organized: {category_file}\")\n",
    "    \n",
    "    # Format 4: High relevance chunks by traveler type\n",
    "    traveler_types = ['solo_traveler', 'family', 'adventure']\n",
    "    for traveler_type in traveler_types:\n",
    "        high_relevance = [chunk for chunk in all_chunks_flat \n",
    "                         if chunk['relevance_scores'][traveler_type] >= 7]\n",
    "        \n",
    "        traveler_file = output_path / f\"high_relevance_{traveler_type}.json\"\n",
    "        with open(traveler_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(high_relevance, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ {traveler_type}: {traveler_file} ({len(high_relevance):,} chunks)\")\n",
    "    \n",
    "    # Format 5: Budget-focused chunks (for price-conscious travelers)\n",
    "    budget_chunks = [chunk for chunk in all_chunks_flat \n",
    "                    if chunk['practical_info']['price_range'] in ['budget', 'mid_range']]\n",
    "    budget_file = output_path / \"budget_focused_chunks.json\"\n",
    "    with open(budget_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(budget_chunks, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Budget focused: {budget_file} ({len(budget_chunks):,} chunks)\")\n",
    "    \n",
    "    # Format 6: Practical info summary (contacts, prices, seasonal)\n",
    "    practical_chunks = [chunk for chunk in all_chunks_flat \n",
    "                       if chunk['practical_info']['has_contact'] or \n",
    "                       len(chunk['practical_info']['prices']) > 0]\n",
    "    practical_file = output_path / \"practical_info_optimized.json\"\n",
    "    with open(practical_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(practical_chunks, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Practical info: {practical_file} ({len(practical_chunks):,} chunks)\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Execute saving\n",
    "output_directory = save_optimized_chunked_data(chunked_data_optimized)\n",
    "print(f\"\\nüìÅ All optimized formats saved to: {output_directory}\")\n",
    "\n",
    "print(f\"\\nüöÄ RAG-READY DATA GENERATED!\")\n",
    "print(\"Available formats:\")\n",
    "print(\"‚Ä¢ all_chunks_optimized.json ‚Üí Vector database with location coordinates\")\n",
    "print(\"‚Ä¢ chunks_by_location.json ‚Üí State/City hierarchical search\")  \n",
    "print(\"‚Ä¢ chunks_by_category_optimized.json ‚Üí Activity-based with subcategories\")\n",
    "print(\"‚Ä¢ high_relevance_[type].json ‚Üí Traveler-specific recommendations\")\n",
    "print(\"‚Ä¢ budget_focused_chunks.json ‚Üí Price-conscious travel planning\")\n",
    "print(\"‚Ä¢ practical_info_optimized.json ‚Üí Contact details and pricing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
