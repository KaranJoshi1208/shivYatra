{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "API_URL = \"https://en.wikivoyage.org/w/api.php\"\n",
    "\n",
    "def fetch_page_html(page_title):\n",
    "    \"\"\"\n",
    "    Fetch HTML content of a WikiVoyage page to extract links\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': page_title,\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': False,  # Get HTML to extract links\n",
    "        'exsectionformat': 'wiki'\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'ShivYatra Travel App (Educational Purpose)'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(API_URL, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        pages = data['query']['pages']\n",
    "        page_id = list(pages.keys())[0]\n",
    "        page_data = pages[page_id]\n",
    "        \n",
    "        if 'extract' in page_data:\n",
    "            return page_data['extract']\n",
    "        return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {page_title}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regions_links(page_title):\n",
    "    \"\"\"\n",
    "    Extract links from the Regions section of a WikiVoyage page\n",
    "    \"\"\"\n",
    "    # First get the plain text to check if regions section exists\n",
    "    page_data = fetch_page_extract(page_title)\n",
    "    if not page_data:\n",
    "        return []\n",
    "    \n",
    "    sections = parse_sections_from_extract(page_data['extract'])\n",
    "    \n",
    "    # Check if Regions section exists\n",
    "    if 'Regions' not in sections:\n",
    "        return []\n",
    "    \n",
    "    # Get HTML content to extract actual links\n",
    "    html_content = fetch_page_html(page_title)\n",
    "    if not html_content:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all internal WikiVoyage links\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        # Look for WikiVoyage internal links\n",
    "        if href.startswith('/wiki/') and ':' not in href:\n",
    "            # Clean up the link\n",
    "            page_name = href.replace('/wiki/', '').replace('_', ' ')\n",
    "            page_name = unquote(page_name)  # Decode URL encoding\n",
    "            links.append(page_name)\n",
    "    \n",
    "    # Filter to get only links that appear in the regions section content\n",
    "    regions_content = sections['Regions'].lower()\n",
    "    regions_links = []\n",
    "    \n",
    "    for link in links:\n",
    "        if link.lower() in regions_content or link.replace(' ', '_').lower() in regions_content:\n",
    "            regions_links.append(link)\n",
    "    \n",
    "    return list(set(regions_links))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddf8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_final_destinations(page_title, visited=None, max_depth=5):\n",
    "    \"\"\"\n",
    "    Recursively find all final destination pages (pages without Regions section)\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    if page_title in visited or max_depth <= 0:\n",
    "        return []\n",
    "    \n",
    "    visited.add(page_title)\n",
    "    print(f\"Exploring: {page_title}\")\n",
    "    \n",
    "    # Get page data\n",
    "    page_data = fetch_page_extract(page_title)\n",
    "    if not page_data:\n",
    "        return []\n",
    "    \n",
    "    sections = parse_sections_from_extract(page_data['extract'])\n",
    "    \n",
    "    # If no Regions section, this is a final destination\n",
    "    if 'Regions' not in sections:\n",
    "        print(f\"  â†’ Final destination found: {page_title}\")\n",
    "        return [page_title]\n",
    "    \n",
    "    # If has Regions section, explore the linked pages\n",
    "    final_destinations = []\n",
    "    regions_links = extract_regions_links(page_title)\n",
    "    \n",
    "    print(f\"  â†’ Found {len(regions_links)} regions to explore\")\n",
    "    \n",
    "    for link in regions_links:\n",
    "        if link not in visited:\n",
    "            final_destinations.extend(\n",
    "                find_final_destinations(link, visited, max_depth - 1)\n",
    "            )\n",
    "    \n",
    "    return final_destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore all states in him_north_list and find final destinations\n",
    "state_destinations = {}\n",
    "\n",
    "print(\"ðŸ” Starting exploration of Himalayan North states...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for state in him_north_list:\n",
    "    print(f\"\\nðŸ”ï¸  EXPLORING STATE: {state}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    destinations = find_final_destinations(state)\n",
    "    state_destinations[state] = destinations\n",
    "    \n",
    "    print(f\"\\nâœ… Found {len(destinations)} final destinations for {state}\")\n",
    "    print(f\"Destinations: {destinations[:5]}{'...' if len(destinations) > 5 else ''}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Exploration complete!\")\n",
    "print(f\"Total states explored: {len(state_destinations)}\")\n",
    "for state, destinations in state_destinations.items():\n",
    "    print(f\"  {state}: {len(destinations)} destinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f06e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate lists for each state (as requested)\n",
    "\n",
    "# Himachal Pradesh destinations\n",
    "himachal_pradesh_destinations = state_destinations.get(\"Himachal_Pradesh\", [])\n",
    "print(f\"Himachal Pradesh destinations ({len(himachal_pradesh_destinations)}):\")\n",
    "print(himachal_pradesh_destinations)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Jammu and Kashmir destinations  \n",
    "jammu_kashmir_destinations = state_destinations.get(\"Jammu_and_Kashmir\", [])\n",
    "print(f\"Jammu and Kashmir destinations ({len(jammu_kashmir_destinations)}):\")\n",
    "print(jammu_kashmir_destinations)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Ladakh destinations\n",
    "ladakh_destinations = state_destinations.get(\"Ladakh\", [])\n",
    "print(f\"Ladakh destinations ({len(ladakh_destinations)}):\")\n",
    "print(ladakh_destinations)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Uttarakhand destinations\n",
    "uttarakhand_destinations = state_destinations.get(\"Uttarakhand\", [])\n",
    "print(f\"Uttarakhand destinations ({len(uttarakhand_destinations)}):\")\n",
    "print(uttarakhand_destinations)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
